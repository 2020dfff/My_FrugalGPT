{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharkiefff/anaconda3/envs/dis-llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# read test data from data/{dataname}/Queried_{dataname}_all_models_clean_test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "\n",
    "sys.path.append('src')\n",
    "\n",
    "from FrugalGPT import optimizer\n",
    "import FrugalGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supported LLMs: ['google/gemini-1.5-flash-002', 'google/gemini-1.5-pro-002', 'google/gemini-1.0-pro', 'openaichat/gpt-4o-mini', 'openaichat/gpt-4o', 'azure/Phi-3-mini-4k-instruct', 'azure/Phi-3.5-mini-instruct', 'azure/Phi-3-small-8k-instruct', 'azure/Phi-3-medium-4k-instruct', 'deepinfra/llama-3-8B', 'deepinfra/llama-3-70B', 'deepinfra/mixtral-8x7B']\n",
      "supported_LLM_names: ['gemini-1.5-flash-002', 'gemini-1.5-pro-002', 'gemini-1.0-pro', 'gpt-4o-mini', 'gpt-4o', 'Phi-3-mini-4k-instruct', 'Phi-3.5-mini-instruct', 'Phi-3-small-8k-instruct', 'Phi-3-medium-4k-instruct', 'llama-3-8B', 'llama-3-70B', 'mixtral-8x7B']\n"
     ]
    }
   ],
   "source": [
    "supported_LLM = FrugalGPT.getservicename()\n",
    "print(\"supported LLMs:\",supported_LLM)\n",
    "supported_LLM_names = [llm.split(\"/\")[1] for llm in supported_LLM]\n",
    "print(\"supported_LLM_names:\", supported_LLM_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_names = ['openaichat/gpt-4o-mini',\n",
    "                'openaichat/gpt-4o',\n",
    "                'google/gemini-1.5-flash-002',\n",
    "                'google/gemini-1.5-pro-002',\n",
    "                'google/gemini-1.0-pro',\n",
    "                'azure/Phi-3-mini-4k-instruct',\n",
    "                'azure/Phi-3.5-mini-instruct',\n",
    "                'azure/Phi-3-small-8k-instruct',\n",
    "                'azure/Phi-3-medium-4k-instruct',\n",
    "                'deepinfra/llama-3-8B',\n",
    "                'deepinfra/llama-3-70B',\n",
    "                'deepinfra/mixtral-8x7B',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_raw</th>\n",
       "      <th>query</th>\n",
       "      <th>ref_answer</th>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <th>gpt-4o</th>\n",
       "      <th>llama-3-8B</th>\n",
       "      <th>llama-3-70B</th>\n",
       "      <th>mixtral-8x7B</th>\n",
       "      <th>gemini-1.5-flash-002</th>\n",
       "      <th>gemini-1.0-pro</th>\n",
       "      <th>gemini-1.5-pro-002</th>\n",
       "      <th>Phi-3.5-mini-instruct</th>\n",
       "      <th>Phi-3-small-8k-instruct</th>\n",
       "      <th>Phi-3-mini-4k-instruct</th>\n",
       "      <th>Phi-3-medium-4k-instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Context: to the extent that these cases are in...</td>\n",
       "      <td>Please determine whether a sentence is overrul...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Context: we therefore reverse the order denyin...</td>\n",
       "      <td>Please determine whether a sentence is overrul...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Context: see brown v. state,\\nQuestion: Is it ...</td>\n",
       "      <td>Please determine whether a sentence is overrul...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Context: at the very least, this court ought t...</td>\n",
       "      <td>Please determine whether a sentence is overrul...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Context: the federal immigration judge and the...</td>\n",
       "      <td>Please determine whether a sentence is overrul...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           query_raw  \\\n",
       "0  Context: to the extent that these cases are in...   \n",
       "1  Context: we therefore reverse the order denyin...   \n",
       "2  Context: see brown v. state,\\nQuestion: Is it ...   \n",
       "3  Context: at the very least, this court ought t...   \n",
       "4  Context: the federal immigration judge and the...   \n",
       "\n",
       "                                               query ref_answer gpt-4o-mini  \\\n",
       "0  Please determine whether a sentence is overrul...        yes         yes   \n",
       "1  Please determine whether a sentence is overrul...        yes         yes   \n",
       "2  Please determine whether a sentence is overrul...         no          no   \n",
       "3  Please determine whether a sentence is overrul...         no          no   \n",
       "4  Please determine whether a sentence is overrul...        yes          no   \n",
       "\n",
       "  gpt-4o llama-3-8B llama-3-70B mixtral-8x7B gemini-1.5-flash-002  \\\n",
       "0    yes        yes         yes          yes                  yes   \n",
       "1    yes        yes         yes          yes                  yes   \n",
       "2     no         no          no           no                   no   \n",
       "3     no         no          no           no                   no   \n",
       "4     no        yes          no           no                  yes   \n",
       "\n",
       "  gemini-1.0-pro gemini-1.5-pro-002 Phi-3.5-mini-instruct  \\\n",
       "0            yes                yes                   yes   \n",
       "1            yes                yes                   yes   \n",
       "2             no                 no                    no   \n",
       "3             no                 no                   yes   \n",
       "4             no                 no                   yes   \n",
       "\n",
       "  Phi-3-small-8k-instruct Phi-3-mini-4k-instruct Phi-3-medium-4k-instruct  \n",
       "0                     yes                    yes                      yes  \n",
       "1                     yes                    yes                      yes  \n",
       "2                      no                     no                       no  \n",
       "3                      no                     no                       no  \n",
       "4                      no                    yes                      yes  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataname = \"OVERRULING\"\n",
    "# dataname = \"AGNEWS\"\n",
    "# dataname = \"HEADLINES\"\n",
    "# read data\n",
    "test_data_df = pd.read_csv(f\"data/{dataname}/Queried_{dataname}_all_models_clean_train.csv\", header=0) # test\n",
    "\n",
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for index, row in test_data_df.iterrows():\n",
    "    query = row['query']\n",
    "    ref_answer = row['ref_answer']\n",
    "    _id = index\n",
    "    model_answer = {}\n",
    "    for model_name in supported_LLM_names:\n",
    "        model_answer[model_name] = row[model_name]\n",
    "    test_data.append([query, ref_answer, _id, model_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please determine whether a sentence is overruling a prior decision (Yes or No) in the following statements.\n",
      "\n",
      "Context: because jones/walker relates only to sufficiency of the evidence, we hereby disavow the language holding otherwise in sandoval.\n",
      "Question: Is it overruling?\n",
      "Answer: Yes\n",
      "\n",
      "Context: according to napa auto parts, the straws drove the vehicle \"\"\"\"for approximately six [] weeks and [] for between 500 to 600 miles prior to the accident with no incidents.\"\"\"\"\n",
      "Question: Is it overruling?\n",
      "Answer: No\n",
      "\n",
      "Context: we therefore reverse the order denying the motion to suppress and recede from any language in moskowitz which could be interpreted contrary to our holding in this case.\n",
      "Question: Is it overruling?\n",
      "Answer:\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "print(test_data_df.iloc[1][\"query\"])\n",
    "# calculate the number of words in the query\n",
    "encoding = tiktoken.get_encoding('cl100k_base')\n",
    "in_token_num = len(encoding.encode(test_data_df.iloc[303][\"query\"]))\n",
    "print(in_token_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.2025462962963\n"
     ]
    }
   ],
   "source": [
    "# calculate the average token length in query column using tiktoken\n",
    "def get_avg_token_length(dataframe):\n",
    "    token_num = 0\n",
    "    for q in dataframe[\"query\"]:\n",
    "        token_num += len(tiktoken.get_encoding('cl100k_base').encode(q))\n",
    "    return token_num / len(dataframe)\n",
    "print(get_avg_token_length(test_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5e-08\n",
      "1.5e-05\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# read from config/serviceinfo_thrift.json, get the price per token for each model\n",
    "\n",
    "with open(\"config/serviceinfo_thrift.json\", \"r\") as f:\n",
    "    serviceinfo = json.load(f)\n",
    "\n",
    "def get_input_price_per_token(model_name):\n",
    "    for service in serviceinfo:\n",
    "        if model_name in serviceinfo[service]:\n",
    "            return serviceinfo[service][model_name][\"cost_input\"]\n",
    "    return None\n",
    "\n",
    "def get_output_price_per_token(model_name):\n",
    "    for service in serviceinfo:\n",
    "        if model_name in serviceinfo[service]:\n",
    "            return serviceinfo[service][model_name][\"cost_output\"]\n",
    "    return None\n",
    "\n",
    "def get_fixed_price(model_name):\n",
    "    for service in serviceinfo:\n",
    "        if model_name in serviceinfo[service]:\n",
    "            return serviceinfo[service][model_name][\"cost_fixed\"]\n",
    "    return None\n",
    "\n",
    "print(get_input_price_per_token(\"gemini-1.5-flash-002\"))\n",
    "print(get_output_price_per_token(\"gpt-4o\"))\n",
    "print(get_fixed_price(\"gpt-4o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cost for openaichat/gpt-4o-mini  is:  2.614583333333338e-05\n",
      "average cost for openaichat/gpt-4o  is:  0.0008665277777777757\n",
      "average cost for google/gemini-1.5-flash-002  is:  1.307291666666669e-05\n",
      "average cost for google/gemini-1.5-pro-002  is:  0.0006065694444444452\n",
      "average cost for google/gemini-1.0-pro  is:  8.665277777777774e-05\n",
      "average cost for azure/Phi-3-mini-4k-instruct  is:  2.2659722222222156e-05\n",
      "average cost for azure/Phi-3.5-mini-instruct  is:  2.2659722222222156e-05\n",
      "average cost for azure/Phi-3-small-8k-instruct  is:  2.614583333333338e-05\n",
      "average cost for azure/Phi-3-medium-4k-instruct  is:  2.9631944444444425e-05\n",
      "average cost for deepinfra/llama-3-8B  is:  9.421805555555558e-06\n",
      "average cost for deepinfra/llama-3-70B  is:  6.0006944444444445e-05\n",
      "average cost for deepinfra/mixtral-8x7B  is:  4.1113333333333386e-05\n"
     ]
    }
   ],
   "source": [
    "data = test_data\n",
    "llm_vanilla = FrugalGPT.llmvanilla.LLMVanilla()\n",
    "\n",
    "# 计算average cost in the test data for each model\n",
    "for name in service_names:\n",
    "    service_name = name\n",
    "    sum_cost = 0\n",
    "    for i in range(len(data)):\n",
    "        query = data[i][0]\n",
    "        cost = llm_vanilla.compute_cost(input_text=query, output_text=\"no\", service_name=service_name)\n",
    "        sum_cost += cost\n",
    "        # print(\"data index is: \", data[i][2], \"and cost for\", service_name, \" is: \", cost)\n",
    "    print(\"average cost for\", service_name, \" is: \", sum_cost/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama-3-8B', 'Phi-3-mini-4k-instruct', 'gpt-4o-mini']\n",
      "[0.02565051282051282, 0.02565051282051282, 0.9486989743589744]\n"
     ]
    }
   ],
   "source": [
    "# calculate the total cost of a cascaded models (depth = 3), by summing the cost of input and output tokens\n",
    "# first read from /strategy/{dataname}_1015/cascade_strategy.json, to get the model_list with corresponding budget\n",
    "# this file is like:\n",
    "# {\n",
    "    # \"budget\": {\n",
    "    #     \"1e-05\": {\n",
    "    #         \"thres_list\": [\n",
    "    #             0.05691420375880523,\n",
    "    #             0.10379833045938076,\n",
    "    #             1.0\n",
    "    #         ],\n",
    "    #         \"model_list\": [\n",
    "    #             \"deepinfra/llama-3-8B\",\n",
    "    #             \"azure/Phi-3-mini-4k-instruct\",\n",
    "    #             \"openaichat/gpt-4o-mini\"\n",
    "    #         ],\n",
    "    #         \"quantile\": [\n",
    "    #             0.02565051282051282,\n",
    "    #             0.02565051282051282\n",
    "    #         ]\n",
    "    #     },\n",
    "    #     \"5e-05\": {\n",
    "\n",
    "    # need to calculate the total cost of a cascaded models (depth = 3), by summing the cost of input and output tokens for each model\n",
    "    # and use the quantile to decide the weight, e.g., 0.02565051282051282 for the first model, 0.02565051282051282 for using both the first and second model, 1-0.02565051282051282-0.02565051282051282 for using all three models\n",
    "    \n",
    "\n",
    "with open(f\"strategy/{dataname}_1015/cascade_strategy.json\", \"r\") as f:\n",
    "    cascade_strategy = json.load(f)\n",
    "\n",
    "def get_model_list(budget):\n",
    "    # split and only need the model name, e.g., from 'openaichat/gpt-4o-mini' to 'gpt-4o-mini'\n",
    "    model_list = cascade_strategy[\"budget\"][budget][\"model_list\"]\n",
    "    return [re.split(\"/\", model)[1] for model in model_list]\n",
    "\n",
    "def get_quantile(budget):\n",
    "    # need to fill up the third value which is 1-quantile[0]-quantile[1]\n",
    "    quantile = cascade_strategy[\"budget\"][budget][\"quantile\"]\n",
    "    quantile.append(1-quantile[0]-quantile[1])\n",
    "    return quantile\n",
    "\n",
    "print(get_model_list(\"1e-05\"))\n",
    "print(get_quantile(\"1e-05\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_cost(dataframe, budget):\n",
    "    # need to calculate the total cost of a cascaded models (depth = 3), by summing the cost of input and output tokens for each model\n",
    "    # and use the quantile to decide the weight, e.g., 0.02565051282051282 for the first model, 0.02565051282051282 for using both the first and second model, 1-0.02565051282051282-0.02565051282051282 for using all three models\n",
    "    model_list = get_model_list(budget)\n",
    "    use_first_cost = (get_input_price_per_token(model_list[0]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[0]) * 1) * get_quantile(budget)[2]\n",
    "    # use_first2_cost = (get_input_price_per_token(model_list[0]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[0]) * 1 \n",
    "    #                    + (get_input_price_per_token(model_list[1]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[1]) * 1)) * get_quantile(budget)[1]\n",
    "    # use_all3_cost = (get_input_price_per_token(model_list[0]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[0]) * 1\n",
    "                        # + (get_input_price_per_token(model_list[1]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[1]) * 1)\n",
    "                        # + (get_input_price_per_token(model_list[2]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[2]) * 1)) * get_quantile(budget)[0]\n",
    "    use_first2_cost = (get_input_price_per_token(model_list[1]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[1]) * 1) * get_quantile(budget)[1]\n",
    "    use_all3_cost = (get_input_price_per_token(model_list[2]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[2]) * 1) * get_quantile(budget)[0]\n",
    "    \n",
    "    total_cost = use_first_cost + use_first2_cost + use_all3_cost\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def get_total_cost_cascaded(dataframe, budget):\n",
    "    model_list = get_model_list(budget)\n",
    "    total_cost = 0\n",
    "    for i in range(len(model_list)):\n",
    "        model = model_list[i]\n",
    "        total_cost += get_input_price_per_token(model) * get_avg_token_length(dataframe)\n",
    "        total_cost += get_output_price_per_token(model) * 1\n",
    "        # if total_cost < float(budget):\n",
    "        print(f\"up to model {model} cost: {total_cost}\")\n",
    "            # continue\n",
    "        # else:\n",
    "            # break\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up to model llama-3-8B cost: 9.361140046296297e-06\n",
      "up to model Phi-3-mini-4k-instruct cost: 3.187747106481482e-05\n",
      "up to model gpt-4o-mini cost: 5.785785300925927e-05\n",
      "5.785785300925927e-05\n"
     ]
    }
   ],
   "source": [
    "print(get_total_cost_cascaded(test_data_df, \"1e-05\"))\n",
    "# print(get_total_cost(test_data_df, \"0.001\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
