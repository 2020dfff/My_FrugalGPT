{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data from data/{dataname}/Queried_{dataname}_all_models_clean_test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "\n",
    "sys.path.append('src')\n",
    "\n",
    "from FrugalGPT import optimizer\n",
    "import FrugalGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supported LLMs: ['google/gemini-1.5-flash-002', 'google/gemini-1.5-pro-002', 'google/gemini-1.0-pro', 'openaichat/gpt-4o-mini', 'openaichat/gpt-4o', 'azure/Phi-3-mini-4k-instruct', 'azure/Phi-3.5-mini-instruct', 'azure/Phi-3-small-8k-instruct', 'azure/Phi-3-medium-4k-instruct', 'deepinfra/llama-3-8B', 'deepinfra/llama-3-70B', 'deepinfra/mixtral-8x7B']\n",
      "supported_LLM_names: ['gemini-1.5-flash-002', 'gemini-1.5-pro-002', 'gemini-1.0-pro', 'gpt-4o-mini', 'gpt-4o', 'Phi-3-mini-4k-instruct', 'Phi-3.5-mini-instruct', 'Phi-3-small-8k-instruct', 'Phi-3-medium-4k-instruct', 'llama-3-8B', 'llama-3-70B', 'mixtral-8x7B']\n"
     ]
    }
   ],
   "source": [
    "supported_LLM = FrugalGPT.getservicename()\n",
    "print(\"supported LLMs:\",supported_LLM)\n",
    "supported_LLM_names = [llm.split(\"/\")[1] for llm in supported_LLM]\n",
    "print(\"supported_LLM_names:\", supported_LLM_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_names = ['openaichat/gpt-4o-mini',\n",
    "                'openaichat/gpt-4o',\n",
    "                'google/gemini-1.5-flash-002',\n",
    "                'google/gemini-1.5-pro-002',\n",
    "                'google/gemini-1.0-pro',\n",
    "                'azure/Phi-3-mini-4k-instruct',\n",
    "                'azure/Phi-3.5-mini-instruct',\n",
    "                'azure/Phi-3-small-8k-instruct',\n",
    "                'azure/Phi-3-medium-4k-instruct',\n",
    "                'deepinfra/llama-3-8B',\n",
    "                'deepinfra/llama-3-70B',\n",
    "                'deepinfra/mixtral-8x7B',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_raw</th>\n",
       "      <th>query</th>\n",
       "      <th>ref_answer</th>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <th>gpt-4o</th>\n",
       "      <th>llama-3-8B</th>\n",
       "      <th>llama-3-70B</th>\n",
       "      <th>mixtral-8x7B</th>\n",
       "      <th>gemini-1.5-flash-002</th>\n",
       "      <th>gemini-1.0-pro</th>\n",
       "      <th>gemini-1.5-pro-002</th>\n",
       "      <th>Phi-3.5-mini-instruct</th>\n",
       "      <th>Phi-3-small-8k-instruct</th>\n",
       "      <th>Phi-3-mini-4k-instruct</th>\n",
       "      <th>Phi-3-medium-4k-instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q: America West Backs Away From ATA Bid Americ...</td>\n",
       "      <td>Please answer which category (World, Sports, B...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q: Compete against your friends, SI experts an...</td>\n",
       "      <td>Please answer which category (World, Sports, B...</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q: Oracle expected to push on content manageme...</td>\n",
       "      <td>Please answer which category (World, Sports, B...</td>\n",
       "      <td>sci/tech</td>\n",
       "      <td>business</td>\n",
       "      <td>sci/tech</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>sci/tech</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>sci/tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q: Bosox strike deal with Mirabelli; Yanks, Fl...</td>\n",
       "      <td>Please answer which category (World, Sports, B...</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>business</td>\n",
       "      <td>sports</td>\n",
       "      <td>business</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q: Bonds deserves a  quot;C quot; for historic...</td>\n",
       "      <td>Please answer which category (World, Sports, B...</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>world</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           query_raw  \\\n",
       "0  Q: America West Backs Away From ATA Bid Americ...   \n",
       "1  Q: Compete against your friends, SI experts an...   \n",
       "2  Q: Oracle expected to push on content manageme...   \n",
       "3  Q: Bosox strike deal with Mirabelli; Yanks, Fl...   \n",
       "4  Q: Bonds deserves a  quot;C quot; for historic...   \n",
       "\n",
       "                                               query ref_answer gpt-4o-mini  \\\n",
       "0  Please answer which category (World, Sports, B...   business    business   \n",
       "1  Please answer which category (World, Sports, B...     sports      sports   \n",
       "2  Please answer which category (World, Sports, B...   sci/tech    business   \n",
       "3  Please answer which category (World, Sports, B...     sports      sports   \n",
       "4  Please answer which category (World, Sports, B...     sports      sports   \n",
       "\n",
       "     gpt-4o llama-3-8B llama-3-70B mixtral-8x7B gemini-1.5-flash-002  \\\n",
       "0  business   business    business     business             business   \n",
       "1    sports     sports      sports       sports               sports   \n",
       "2  sci/tech   business    business     business             business   \n",
       "3    sports     sports      sports       sports               sports   \n",
       "4    sports     sports      sports       sports               sports   \n",
       "\n",
       "  gemini-1.0-pro gemini-1.5-pro-002 Phi-3.5-mini-instruct  \\\n",
       "0       business           business              business   \n",
       "1         sports             sports                sports   \n",
       "2       business           sci/tech              business   \n",
       "3         sports             sports              business   \n",
       "4         sports             sports                 world   \n",
       "\n",
       "  Phi-3-small-8k-instruct Phi-3-mini-4k-instruct Phi-3-medium-4k-instruct  \n",
       "0                business               business                 business  \n",
       "1                  sports                 sports                   sports  \n",
       "2                business               business                 sci/tech  \n",
       "3                  sports               business                   sports  \n",
       "4                  sports                 sports                   sports  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataname = \"OVERRULING\"\n",
    "dataname = \"AGNEWS\"\n",
    "# dataname = \"HEADLINES\"\n",
    "# read data\n",
    "test_data_df = pd.read_csv(f\"data/{dataname}/Queried_{dataname}_all_models_clean_test.csv\", header=0)\n",
    "\n",
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for index, row in test_data_df.iterrows():\n",
    "    query = row['query']\n",
    "    ref_answer = row['ref_answer']\n",
    "    _id = index\n",
    "    model_answer = {}\n",
    "    for model_name in supported_LLM_names:\n",
    "        model_answer[model_name] = row[model_name]\n",
    "    test_data.append([query, ref_answer, _id, model_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please answer which category (World, Sports, Business or Sci/Tech) a provided news follows into.\n",
      "\n",
      "Q: Five-year ban for Blackburn fan One of the two Blackburn Rovers Football Club fans charged with public disorder for racially abusing Dwight Yorke has been handed a five-year ban.\n",
      "A: Sports\n",
      "\n",
      "Q: Major software pirates caught A multimillion-euro software piracy ring has been broken following synchronized raids in Athens and London yesterday, Attica police said.\n",
      "A: Sci/Tech\n",
      "\n",
      "Q: Compete against your friends, SI experts and celebrities in this &lt;b&gt;...&lt;/b&gt; OWINGS MILLS, Maryland (Ticker) --  quot;Prime Time quot; has decided this is the right time to return to the NFL. Deion Sanders, regarded as perhaps the most electrifying cornerback in league history, arrived here \n",
      "A:\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "print(test_data_df.iloc[1][\"query\"])\n",
    "# calculate the number of words in the query\n",
    "encoding = tiktoken.get_encoding('cl100k_base')\n",
    "in_token_num = len(encoding.encode(test_data_df.iloc[303][\"query\"]))\n",
    "print(in_token_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.53684210526316\n"
     ]
    }
   ],
   "source": [
    "# calculate the average token length in query column using tiktoken\n",
    "def get_avg_token_length(dataframe):\n",
    "    token_num = 0\n",
    "    for q in dataframe[\"query\"]:\n",
    "        token_num += len(tiktoken.get_encoding('cl100k_base').encode(q))\n",
    "    return token_num / len(dataframe)\n",
    "print(get_avg_token_length(test_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5e-08\n",
      "1.5e-05\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# read from config/serviceinfo_thrift.json, get the price per token for each model\n",
    "\n",
    "with open(\"config/serviceinfo_thrift_actual.json\", \"r\") as f:\n",
    "    serviceinfo = json.load(f)\n",
    "\n",
    "def get_input_price_per_token(model_name):\n",
    "    for service in serviceinfo:\n",
    "        if model_name in serviceinfo[service]:\n",
    "            return serviceinfo[service][model_name][\"cost_input\"]\n",
    "    return None\n",
    "\n",
    "def get_output_price_per_token(model_name):\n",
    "    for service in serviceinfo:\n",
    "        if model_name in serviceinfo[service]:\n",
    "            return serviceinfo[service][model_name][\"cost_output\"]\n",
    "    return None\n",
    "\n",
    "def get_fixed_price(model_name):\n",
    "    for service in serviceinfo:\n",
    "        if model_name in serviceinfo[service]:\n",
    "            return serviceinfo[service][model_name][\"cost_fixed\"]\n",
    "    return None\n",
    "\n",
    "print(get_input_price_per_token(\"gemini-1.5-flash-002\"))\n",
    "print(get_output_price_per_token(\"gpt-4o\"))\n",
    "print(get_fixed_price(\"gpt-4o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data index is:  0 and cost for openaichat/gpt-4o-mini  is:  2.325e-05\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata index is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, data[i][\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand cost for\u001b[39m\u001b[38;5;124m\"\u001b[39m, service_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, cost)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 计算average cost in the test data for each model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage cost for\u001b[39m\u001b[38;5;124m\"\u001b[39m, service_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(cost) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(cost))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "data = test_data\n",
    "llm_vanilla = FrugalGPT.llmvanilla.LLMVanilla()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for name in service_names:\n",
    "        service_name = name\n",
    "        query = data[i][0]\n",
    "        cost = llm_vanilla.compute_cost(input_text=query, output_text=\"no\", service_name=service_name)\n",
    "        print(\"data index is: \", data[i][2], \"and cost for\", service_name, \" is: \", cost)\n",
    "        # 计算average cost in the test data for each model\n",
    "        print(\"average cost for\", service_name, \" is: \", sum(cost) / len(cost))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama-3-8B', 'Phi-3-medium-4k-instruct', 'Phi-3.5-mini-instruct']\n",
      "[0.21643250430689104, 0.4379214029447115, 0.3456460927483974]\n"
     ]
    }
   ],
   "source": [
    "# calculate the total cost of a cascaded models (depth = 3), by summing the cost of input and output tokens\n",
    "# first read from /strategy/{dataname}_1015/cascade_strategy.json, to get the model_list with corresponding budget\n",
    "# this file is like:\n",
    "# {\n",
    "    # \"budget\": {\n",
    "    #     \"1e-05\": {\n",
    "    #         \"thres_list\": [\n",
    "    #             0.05691420375880523,\n",
    "    #             0.10379833045938076,\n",
    "    #             1.0\n",
    "    #         ],\n",
    "    #         \"model_list\": [\n",
    "    #             \"deepinfra/llama-3-8B\",\n",
    "    #             \"azure/Phi-3-mini-4k-instruct\",\n",
    "    #             \"openaichat/gpt-4o-mini\"\n",
    "    #         ],\n",
    "    #         \"quantile\": [\n",
    "    #             0.02565051282051282,\n",
    "    #             0.02565051282051282\n",
    "    #         ]\n",
    "    #     },\n",
    "    #     \"5e-05\": {\n",
    "\n",
    "    # need to calculate the total cost of a cascaded models (depth = 3), by summing the cost of input and output tokens for each model\n",
    "    # and use the quantile to decide the weight, e.g., 0.02565051282051282 for the first model, 0.02565051282051282 for using both the first and second model, 1-0.02565051282051282-0.02565051282051282 for using all three models\n",
    "    \n",
    "\n",
    "with open(f\"strategy/{dataname}_1015/cascade_strategy.json\", \"r\") as f:\n",
    "    cascade_strategy = json.load(f)\n",
    "\n",
    "def get_model_list(budget):\n",
    "    # split and only need the model name, e.g., from 'openaichat/gpt-4o-mini' to 'gpt-4o-mini'\n",
    "    model_list = cascade_strategy[\"budget\"][budget][\"model_list\"]\n",
    "    return [re.split(\"/\", model)[1] for model in model_list]\n",
    "\n",
    "def get_quantile(budget):\n",
    "    # need to fill up the third value which is 1-quantile[0]-quantile[1]\n",
    "    quantile = cascade_strategy[\"budget\"][budget][\"quantile\"]\n",
    "    quantile.append(1-quantile[0]-quantile[1])\n",
    "    return quantile\n",
    "\n",
    "print(get_model_list(\"1e-05\"))\n",
    "print(get_quantile(\"0.0005\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_cost(dataframe, budget):\n",
    "    # need to calculate the total cost of a cascaded models (depth = 3), by summing the cost of input and output tokens for each model\n",
    "    # and use the quantile to decide the weight, e.g., 0.02565051282051282 for the first model, 0.02565051282051282 for using both the first and second model, 1-0.02565051282051282-0.02565051282051282 for using all three models\n",
    "    model_list = get_model_list(budget)\n",
    "    use_first_cost = (get_input_price_per_token(model_list[0]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[0]) * 1) * get_quantile(budget)[2]\n",
    "    # use_first2_cost = (get_input_price_per_token(model_list[0]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[0]) * 1 \n",
    "    #                    + (get_input_price_per_token(model_list[1]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[1]) * 1)) * get_quantile(budget)[1]\n",
    "    # use_all3_cost = (get_input_price_per_token(model_list[0]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[0]) * 1\n",
    "                        # + (get_input_price_per_token(model_list[1]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[1]) * 1)\n",
    "                        # + (get_input_price_per_token(model_list[2]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[2]) * 1)) * get_quantile(budget)[0]\n",
    "    use_first2_cost = (get_input_price_per_token(model_list[1]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[1]) * 1) * get_quantile(budget)[1]\n",
    "    use_all3_cost = (get_input_price_per_token(model_list[2]) * get_avg_token_length(dataframe) + get_output_price_per_token(model_list[2]) * 1) * get_quantile(budget)[0]\n",
    "    \n",
    "    total_cost = use_first_cost + use_first2_cost + use_all3_cost\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def get_total_cost_cascaded(dataframe, budget):\n",
    "    model_list = get_model_list(budget)\n",
    "    total_cost = 0\n",
    "    for i in range(len(model_list)):\n",
    "        model = model_list[i]\n",
    "        total_cost += get_input_price_per_token(model) * get_avg_token_length(dataframe)\n",
    "        total_cost += get_output_price_per_token(model) * 1\n",
    "        if total_cost < float(budget):\n",
    "            print(f\"up to model {model} cost: {total_cost}\")\n",
    "            continue\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up to model llama-3-8B cost: 8.499526315789475e-06\n",
      "3.528078947368421e-05\n"
     ]
    }
   ],
   "source": [
    "print(get_total_cost_cascaded(test_data_df, \"1e-05\"))\n",
    "# print(get_total_cost(test_data_df, \"0.001\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
